[{"path":"index.html","id":"about","chapter":"1 About","heading":"1 About","text":"Portfolio provides archivable record three activities daily journals STAT 764: Applied Spatio-temporal Statistics, Spring 2024.","code":""},{"path":"st-class---january-18th.html","id":"st-class---january-18th","chapter":"1st class - January 18th","heading":"1st class - January 18th","text":"Something new learned class within past 24 hours:\nrecent class session assigned readings (pages 1-15 Wikle et al., 2019), gained profound understanding significance spatial-temporal analysis relevance various fields, particularly within agricultural world located. Basically, data collect temporal spatial component included, underscores importance delving spatial-temporal analysis extract key insights. addition , excited potential applications spatial-temporal analysis work look forward incorporating insights future projects master’s student.Something new learned class within past 24 hours:\nrecent class session assigned readings (pages 1-15 Wikle et al., 2019), gained profound understanding significance spatial-temporal analysis relevance various fields, particularly within agricultural world located. Basically, data collect temporal spatial component included, underscores importance delving spatial-temporal analysis extract key insights. addition , excited potential applications spatial-temporal analysis work look forward incorporating insights future projects master’s student.Something struggling understand covered class within past 24 hours:\nearly stage semester, following introductory class session, find without specific challenges difficulties understanding material. initial class primarily focused providing overview setting stage upcoming topics explore throughout semester. involved insightful explanations structure expectations course.Something struggling understand covered class within past 24 hours:\nearly stage semester, following introductory class session, find without specific challenges difficulties understanding material. initial class primarily focused providing overview setting stage upcoming topics explore throughout semester. involved insightful explanations structure expectations course.","code":""},{"path":"nd-class---january-25th.html","id":"nd-class---january-25th","chapter":"2nd class - January 25th","heading":"2nd class - January 25th","text":"Something new learned class within past 24 hours:One insightful aspect today’s class marathon example, served valuable tool revisiting polynomial regression concepts, also offer comprehensive perspective model assumptions, performance, examination line best fit ensure alignment real-world observations.Something struggling understand covered class within past 24 hours:Regarding something struggling understand probability distributions mathematical models, since never taken mathematical/basic stat classes (first one STAT 705). opinion something need invest time get better understanding consolidate basic principles essential statistical models.opinion important points statistical models need improve sure.","code":""},{"path":"rd-class---january-30th.html","id":"rd-class---january-30th","chapter":"3rd class - January 30th","heading":"3rd class - January 30th","text":"Something new learned class within past 24 hours:Today’s class assigned readings (pages 11-14 Wikle et al., 2019) helped get better understanding concept Hierarchical Statistical Models, defined, works. well covered flexible terms assumptions manipulation aiming improve model performance. addition , ability portray uncertainty rather just estimation also key point opinion. additional information might help improve statistical model, use make better inferences? Another interesting point dealing space-time data basic approach won’t work unless really specify important assumptions (allowed sophisticated models) able improve model performance consequently make meaningful inferences.Something struggling understand covered class within past 24 hours:struggling understand point , can use assumptions change model performance? mean, exactly works, supposed add additional information prior? related/considered selection terms probability distribution given prior? understand topics fully covered far, looking forward attending next classes getting know .","code":""},{"path":"th-class---february-1st.html","id":"th-class---february-1st","chapter":"4th class - February 1st","heading":"4th class - February 1st","text":"Something new learned class within past 24 hours:think explanation change model assumptions great, example using marathon data fitting model helped lot, least mind clearer now can manage . addition , say point removing outliers kind new way covered, mean thinking need take account error aspect might affecting data, odd points. Lastly, understand better prediction, forecasts, hindcasts mean model purpose.Something struggling understand covered class within past 24 hours:struggling understand point, adding complexity model might help predicting forecasting hindcasting. makes miss ability extrapolate beyond range data, words, explanation happening. Communicating uncertainty another key point need get better understanding .","code":""},{"path":"th-class---february-6th.html","id":"th-class---february-6th","chapter":"5th class - February 6th","heading":"5th class - February 6th","text":"Something new learned class within past 24 hours:Today’s class nice oportunity review basic concepts probability distribution function, mainly normal distribution. adition , make function PDF (using exponential distribution example) something new , think helped get better understanding move mathematical equations function PDF distribution. difference mean expected value using maximum “likeliehood” approach might something simpels never heard , great start thinking understanding deeply.Something struggling understand covered class within past 24 hours:’d say part class mathematical model rivew, specially regarding difference equations (works discrete functions) differential equations (works continuous) clear need look materials study get familiar . Something need explore write mathematical equations R, programming skill need practice improve .","code":""},{"path":"th-class---february-8th.html","id":"th-class---february-8th","chapter":"6th class - February 8th","heading":"6th class - February 8th","text":"Something new learned class within past 24 hours:Something new learned class within past 24 hours Bayesian Hierarchical modeling framework, wasn’t entirely new , found understanding improved significantly. concepts became clearer, feel confident ability use/apply framework data analysis. addition , exactly idea behind posterior distribution (uncertainty parameters) posterior predictive distribution (making predictions future observations based updated knowledge) well covered opinion.Something struggling understand covered class within past 24 hours:Since last class covered review mathematical models explored example regarding whooping crane, didn’t encounter specific struggles, one area piqued curiosity left pondering process selecting distribution particular data model. find wondering criteria consider making decision can test different distributions arrive appropriate one.","code":""},{"path":"th-class---february-13th.html","id":"th-class---february-13th","chapter":"7th class - February 13th","heading":"7th class - February 13th","text":"Something new learned class within past 24 hours:Today’s class great better understanding Data model, Process model, Parameter model hierarchical framework. addition , now clearer can add assumptions model, choosing distributions layer hierarchical model reflects real data. support expected value observed value also something new today’s class, opinion, important step, spend time beginning data analysis process consider unique information regarding data help later selecting appropriate distribution.Something struggling understand covered class within past 24 hours:Something struggling understand dynamic mathematical model example covered class, well get idea behind dependence births, deaths, growth rate beginning, written, get analytical solution, something clear head point.","code":""},{"path":"th-class---february-15th.html","id":"th-class---february-15th","chapter":"8th class - February 15th","heading":"8th class - February 15th","text":"Something new learned class within past 24 hours:Something new today’s class can work Parameter Model (prior), Whooping crane examples set reasonable priors model, great better understand process works consider. addition , simulating data prior predictive distribution rejection sampling new .Something struggling understand covered class within past 24 hours:Something struggling understand R coding part, mean adding mathematical model selected distribution R. say codes seem okay, can understand, specific points like mathematical ones familiar .","code":""},{"path":"th-class---february-20th.html","id":"th-class---february-20th","chapter":"9th class - February 20th","heading":"9th class - February 20th","text":"Something new learned class within past 24 hours:’d say something new learned class within past 24 hours can download weather (rainfall) data. addition , features sf package also new . idea first setting goals wanna make inferences start analyzing data important point, mentioned previous classes, always good keep mind clear objectives exploring deeply dataset.Something struggling understand covered class within past 24 hours:Something struggling understand deal NA, implications indeed choice. Mainly extreme rainfall example, topic came , point view, clear best/preferable approach scenario.","code":""},{"path":"th-class---february-22th.html","id":"th-class---february-22th","chapter":"10th class - February 22th","heading":"10th class - February 22th","text":"Something new learned class within past 24 hours:Something new learned class regarding intro GIS, ’ve using GIS data like shapefiles points, however, review today’s class great better interpret . addition , transformations coordinate reference system discussion also important identify potential units used world deal .Something struggling understand covered class within past 24 hours:Something struggling understand Gaussian process part class, opinion, fast enough time explore , since important aspect present working Hierarchical Bayesian Framework.","code":""},{"path":"th-class---february-27th.html","id":"th-class---february-27th","chapter":"11th class - February 27th","heading":"11th class - February 27th","text":"Something new learned class within past 24 hours:Something new learned today’s class multivariate normal distribution can use spatial-temporal analysis. addition , part covering potential correlation matrices included multivariate normal distribution great better understand correlation suitable case based correlation among variables.Something struggling understand covered class within past 24 hours:Something struggling understand Gaussian correlation function, mean get basically mathematical function helps us describe two variables correlated, interpret results based output got something need read understand better.","code":""},{"path":"th-class---february-29th.html","id":"th-class---february-29th","chapter":"12th class - February 29th","heading":"12th class - February 29th","text":"Something new learned class within past 24 hours:’d say today’s class great revising/learning apply hierarchical model spatial analysis, mainly extreme rainfall example covered class. Starting exploratory data analysis (steps explore) going statistical data analysis, testing different models, hierarchical linear model two error terms, comparing non-hierarchical approach. addition , make spatial predictions pixel something new .Something struggling understand covered class within past 24 hours:Something struggling understand properly write model inside function, specified parameters distributions. example extreme rainfall analysis, translate data model process model gls function R.","code":""},{"path":"th-class---march-5th.html","id":"th-class---march-5th","chapter":"13th class - March 5th","heading":"13th class - March 5th","text":"Something new learned class within past 24 hours:Today’s class great opportunity practice concepts ’ve seen last couple weeks, mainly applying activity 2. learned fit simple kriging model elevation dataset, working create elevation map field trial, aiming get better insights place blocks avoid areas low elevation. addition , cool see many different models friends trying implement elevation data.Something struggling understand covered class within past 24 hours:Something struggling fitting second order polynomial regression, guess model converging, get NA second-order term. ’m trying fix , otherwise plan stop office hours week.","code":""},{"path":"th-class---march-7th.html","id":"th-class---march-7th","chapter":"14th class - March 7th","heading":"14th class - March 7th","text":"Something new learned class within past 24 hours:Today’s class great explore learn alternatives modeling elevation data. saw many different approaches, regression trees, gradient boosting, support vector machines.Something struggling understand covered class within past 24 hours:Since class practicing working activity 2, anything special emphasize something struggling .","code":""},{"path":"th-class---march-19th.html","id":"th-class---march-19th","chapter":"15th class - March 19th","heading":"15th class - March 19th","text":"Something new learned class within past 24 hours:today’s class, review covered far, distributions, hierarchical modeling, model building process. ’d say learned class within past 24 hours mostly related mgcv package, use , applications presents.Something struggling understand covered class within past 24 hours:Something struggling deciding/choosing appropriate PDFs PMFs data, parameters models, think following classes, cover examples , going help clarify get better understanding.","code":""},{"path":"th-class---march-21st.html","id":"th-class---march-21st","chapter":"16th class - March 21st","heading":"16th class - March 21st","text":"Something new learned class within past 24 hours:today’s class, opportunity work activity 2. say something new learned class within past 24 hours mainly related fit kriging model using gam function mgcv package.Something struggling understand covered class within past 24 hours:’d say something still struggling dealing GIS objects making predictions.","code":""},{"path":"th-class---march-26th.html","id":"th-class---march-26th","chapter":"17th class - March 26th","heading":"17th class - March 26th","text":"Something new learned class within past 24 hours:’d say something new learned class within past 24 hours techeniques model checking/comparison, mainly idea collecting data using part data used model fitting check predictive accuracy. addition , scoring rules presented performed today’s class great remember practice model comparison.Something struggling understand covered class within past 24 hours:Something struggling scoring rule appropiate depending model fitting, ’d say something need invest time read .","code":""},{"path":"th-class---march-28th.html","id":"th-class---march-28th","chapter":"18th class - March 28th","heading":"18th class - March 28th","text":"Something new learned class within past 24 hours:today’s class, work class project, something new learned run Generalized Additive Model using Bayesian, brms package R, seems simple implement give us powerful output, including uncertainty, something harder get running GAM mgcv package. addition , learned add block effect GAM model check difference model fitting without .Something struggling understand covered class within past 24 hours:’d say something struggling understanding difference outputs depending function use get predictions, predict(), posterior_predict().","code":""},{"path":"th-class---april-2nd.html","id":"th-class---april-2nd","chapter":"19th class - April 2nd","heading":"19th class - April 2nd","text":"Something new learned class within past 24 hours:’d say something new learned class within past 24 hours deal spatial-temporal models disease data, mean example today’s class great opportunity understand base rules think perform analysis, considering spatial-temporal component factors might impact result, data regarding surrounding areas.Something struggling understand covered class within past 24 hours:struggling understand every piece three different empirical hierarchical models used model fitting Bird Cherry-oat Aphid example.","code":""},{"path":"th-class---april-4th.html","id":"th-class---april-4th","chapter":"20th class - April 4th","heading":"20th class - April 4th","text":"Something new learned class within past 24 hours:today’s class, work class project, something new learned use machine learning algorithm (extreme gradient boosting) assess features importance. Using xgboost algorithm select 5 important variables driving corn yield response nitrogen nutrition index. addition , used multinomial logistic model predict probabilities different possible clusters based type response, flat, plateau response, non-plateau response.Something struggling understand covered class within past 24 hours:’d say something struggling improve multinomial logistic model outcome, able show effect important variables selected xgboost within cluster.","code":""},{"path":"st-class---april-9th.html","id":"st-class---april-9th","chapter":"21st class - April 9th","heading":"21st class - April 9th","text":"Something new learned class within past 24 hours:Something new learned class within past 24 hours mostly related spatio-temporal models used model fitting Bird cherry-oat aphid abundance example. class great opportunity get better understanding piece model, especially first second one, using Poisson negative binomial distributions respectively, also understand better reason behind testing two distributions make sense kind data.Something struggling understand covered class within past 24 hours:’d say something struggling understanding third model tested bird cherry-oat aphid example.","code":""},{"path":"nd-class---april-11th.html","id":"nd-class---april-11th","chapter":"22nd class - April 11th","heading":"22nd class - April 11th","text":"Something new learned class within past 24 hours:today’s class, opportunity work activity 3. say something new learned class within past 24 hours mainly related fit three different spatio-temporal models covered class abundance English grain aphid data. addition , also measure performance predictions model.Something struggling understand covered class within past 24 hours:’d say something struggling regarding concurvity measurement model. something need double check read get know better.","code":""},{"path":"th-class---april-23th.html","id":"th-class---april-23th","chapter":"25th class - April 23th","heading":"25th class - April 23th","text":"Something new learned class within past 24 hours:Today’s class great opportunity work spatio-temporal model fitting, using Earthquake data KS oil gas well data. ’d say something new learned regarding use IPPP (Inhomogeneous Poisson Point Process) modeling spatio-temporal data like Earthquake.Something struggling understand covered class within past 24 hours:’d say something struggling understanding difference first second model performed analyses, mind seem use spatio-temporal effect intine model, second model.","code":""},{"path":"th-class---april-25th.html","id":"th-class---april-25th","chapter":"26th class - April 25th","heading":"26th class - April 25th","text":"Something new learned class within past 24 hours:today’s class, opportunity work activity 3. Mainly checking model fitting results measuring performance predictions model.Something struggling understand covered class within past 24 hours:’d say something struggling writing three statistical models using appropriate notation describing component model using words.","code":""},{"path":"th-class---april-30th.html","id":"th-class---april-30th","chapter":"27th class - April 30th","heading":"27th class - April 30th","text":"Something new learned class within past 24 hours:’d say something new learned class within past 24 hours mostly related use Mechanistic models tool long-range forecasting within spatio-temporal analysis.Something struggling understand covered class within past 24 hours:Regarding something struggling understand, ’d say point specific topic highlight .","code":""},{"path":"activity-1.html","id":"activity-1","chapter":"4 Activity 1","heading":"4 Activity 1","text":"","code":""},{"path":"activity-1.html","id":"library","chapter":"4 Activity 1","heading":"4.1 Library","text":"","code":"\nlibrary(sf)\nlibrary(lubridate) \nlibrary(mgcv)\nlibrary(dplyr)\nlibrary(ggmap)\nlibrary(geosphere)\nlibrary(leaflet)"},{"path":"activity-1.html","id":"read-data-.gpx-file-collected-using-app-strava","chapter":"4 Activity 1","heading":"4.2 Read data (.gpx file) collected using app Strava","text":"","code":"\nurl <- 'https://www.dropbox.com/scl/fi/mcotiucy8hfne8ad2wclp/Afternoon_Ride.gpx?rlkey=25hlqa7o7ug5zvevg3wmf34jc&dl=1'\n\ndata <- st_read(dsn=url,layer=\"track_points\")\n\n\ndata$time <- as.character(data$time)\n\ndata <- data[,5]\n\ndata$time <- as_datetime(data$time,tz=\"America/Chicago\")\n\nprint(data)"},{"path":"activity-1.html","id":"plot-movement-data","chapter":"4 Activity 1","heading":"4.3 Plot movement data","text":"","code":"\n# Create data frame \n\ndf_ride <- data.frame(t = as.numeric(data$time - data$time[1]),\n                          s1 = st_coordinates(data)[,1],\n                          s2 = st_coordinates(data)[,2])\n# Option 01.\n\nggplot()+\n  geom_point(data = df_ride, aes(s1, s2), size = 1, color = \"black\")+\n  labs(x = \"Longitude\", y = \"Latitude\")+\n  theme_linedraw()"},{"path":"activity-1.html","id":"explore-movement-data","chapter":"4 Activity 1","heading":"4.4 Explore movement data","text":"\ndata looks okay, point beginning, parking lot, presents large error location, besides , looks great. got surprised captured side road driving, expect .","code":""},{"path":"activity-1.html","id":"fit-model-to-movement-data","chapter":"4 Activity 1","heading":"4.5 Fit model to movement data","text":"","code":"\n# Fit model to longitude (s_1)\n\n# a. Polynomial regression\n\nlong_m1 <- lm(s1 ~ poly(t,degree=10,raw=TRUE),data=df_ride)\n\nsummary(long_m1)\n\n\n# b. Generalized additive model\n\nlong_m2 <- gam(s1 ~ s(t,bs=\"gp\",k=50),data=df_ride)\n\nsummary(long_m2)\n\n# Fit model to latitude (s_1) \n\n# a. Polynomial regression\n\nlat_m1 <- lm(s2 ~ poly(t,degree=10,raw=TRUE),data=df_ride)\n\nsummary(lat_m1)\n\n# a. Generalized additive model\n\nlat_m2 <- gam(s2 ~ s(t,bs=\"gp\",k=50),data=df_ride)\n\nsummary(lat_m2)\n\n# Obtain prediction of my location (for every 1 sec)\n\ndf.pred <- data.frame(t = seq(0,as.integer(max(df_ride$t)),by=1)) \n\n\ndf.pred$s1_m1.hat <- predict(long_m1,newdata=df.pred) \ndf.pred$s1_m2.hat <- predict(long_m2,newdata=df.pred)\ndf.pred$s2_m1.hat <- predict(lat_m1,newdata=df.pred) \ndf.pred$s2_m2.hat <- predict(lat_m2,newdata=df.pred)"},{"path":"activity-1.html","id":"plot-estimated-movement-trajectory-from-5","chapter":"4 Activity 1","heading":"4.6 Plot estimated movement trajectory from (5)","text":"","code":"\n  ggplot()+\n  geom_point(data = df_ride, aes(t/60, s1), size = 3)+\n  geom_line(data = df.pred, aes(t/60, s1_m1.hat), color = \"orange\", size = 1)+\n   labs(x = \"Time (minutes)\", y = \"Longitude\", title = \"1.Ponylomial regression model - Longitude\")+\n    theme_linedraw()\n  ggplot()+\n  geom_point(data = df_ride, aes(t/60, s1), size = 3)+\n  geom_line(data = df.pred, aes(t/60, s1_m2.hat), color = \"blue3\", size = 1)+\n    labs(x = \"Time (minutes)\", y = \"Longitude\", title = \"2.Generalized additive model - Longitude\")+\n    theme_linedraw()\n  ggplot()+\n  geom_point(data = df_ride, aes(t/60, s2), size = 3)+\n  geom_line(data = df.pred, aes(t/60, s2_m1.hat), color = \"orange\", size = 1)+\n      labs(x = \"Time (minutes)\", y = \"Latitude\", title = \"3.Polynomial regression model - Latitude\")+\n    theme_linedraw()\n  ggplot()+\n  geom_point(data = df_ride, aes(t/60, s2), size = 3)+\n  geom_line(data = df.pred, aes(t/60, s2_m2.hat), color = \"blue3\", size = 1)+\n        labs(x = \"Time (minutes)\", y = \"Longitude\", title = \"4.Generalized additive model - Latitude\")+\n    theme_linedraw()"},{"path":"activity-1.html","id":"explore-estimated-trajectory","chapter":"4 Activity 1","heading":"4.6.1 Explore estimated trajectory","text":"","code":"\nggplot()+\n  geom_point(data = df_ride, aes(s1, s2), size = 3)+\n  geom_point(data = df.pred, aes(s1_m1.hat, s2_m1.hat), color = \"orange\", size = 1)+\n  labs(x = \"Longitude\", y = \"Latitude\", title = \"Polynomial regression model\")+\n  theme_linedraw()\nggplot()+\n  geom_point(data = df_ride, aes(s1, s2), size = 3)+\n  geom_point(data = df.pred, aes(s1_m2.hat, s2_m2.hat), color = \"blue3\", size = 1)+\n  labs(x = \"Longitude\", y = \"Latitude\", title = \"Generalized additive model\")+\n  theme_linedraw()"},{"path":"activity-1.html","id":"estimate-velocity","chapter":"4 Activity 1","heading":"4.7 Estimate velocity","text":"","code":"\n# 1.1 Calculate speed - observed data\n\ndf_obs <- st_as_sf(df_ride, coords = c(\"s1\", \"s2\"), \n                           crs = st_crs(data))\n\ndist_obs<- st_distance(df_obs$geometry[1:441], df_obs$geometry[2:442], by_element = T)\n(sum(dist_obs)/1000)*.62## 2.881934 [m]\nspeed_obs <- (dist_obs/as.numeric(diff(df_obs$t)))*2.24\nplot(df_obs$t[-1]/60, speed_obs,xlab=\"Time (minutes)\",ylab=\"Velocity (miles per hour)\", main = 'Observed data')\n# 1. Convert model coordinates to sf object\n\ndata.hat.m1 <- st_as_sf(df.pred, coords = c(\"s1_m1.hat\", \"s2_m1.hat\"), \n                           crs = st_crs(data))\n\ndata.hat.m2 <- st_as_sf(df.pred, coords = c(\"s1_m2.hat\", \"s2_m2.hat\"), \n                           crs = st_crs(data))\n\n\n# 1.1 Calculate speed m1\ndist.hat.m1 <- st_distance(data.hat.m1$geometry[1:465], data.hat.m1$geometry[2:466], by_element = T)\n(sum(dist.hat.m1)/1000)*.62## 2.890516 [m]\nspeed.hat.m1 <- (dist.hat.m1/as.numeric(diff(data.hat.m1$t)))*2.24\nplot(data.hat.m1$t[-1]/60, speed.hat.m1,xlab=\"Time (minutes)\",ylab=\"Velocity (miles per hour)\", main = 'Polynomial regression model')\n# 1.2 Calculate speed m2\ndist.hat.m2 <- st_distance(data.hat.m2$geometry[1:465], data.hat.m2$geometry[2:466], by_element = T)\n(sum(dist.hat.m2)/1000)*.62## 2.802154 [m]\nspeed.hat.m2 <- (dist.hat.m2/as.numeric(diff(data.hat.m2$t)))*2.24\nplot(data.hat.m2$t[-1]/60, speed.hat.m2,xlab=\"Time (minutes)\",ylab=\"Velocity (miles per hour)\", main = 'Generalized additive model')"},{"path":"activity-2.html","id":"activity-2","chapter":"5 Activity 2","heading":"5 Activity 2","text":"","code":""},{"path":"activity-2.html","id":"library-1","chapter":"5 Activity 2","heading":"5.1 Library","text":"","code":"\nlibrary(sf)\nlibrary(sp)\nlibrary(raster)\nlibrary(ggplot2)\nlibrary(nlme)\nlibrary(mapview)\nlibrary(dplyr)\nlibrary(gstat)\nlibrary(mgcv)"},{"path":"activity-2.html","id":"chose-an-area-on-or-close-to-campus-where-it-is-easy-for-you-to-understand-how-the-elevation-changes.-using-a-smartphone-record-the-elevation-at-several-locations-points-within-the-area-you-chose.","chapter":"5 Activity 2","heading":"5.2 Chose an area on or close to campus where it is easy for you to understand how the elevation changes. Using a smartphone record the elevation at several locations (points) within the area you chose.","text":"Area: Agronomy North Farm (2201 North Farm - Research Center)","code":""},{"path":"activity-2.html","id":"obtain-a-.gpx-or-.csv-file-for-your-elevation-data.-at-minimum-the-file-should-contain-the-location-and-time-of-the-elevation-measurements.","chapter":"5 Activity 2","heading":"5.3 Obtain a .gpx or .csv file for your elevation data. At minimum the file should contain the location and time of the elevation measurements.","text":"Data obtained using app Strava.","code":"\n# Download shapefile of Kansas from census.gov\ndownload.file(\"http://www2.census.gov/geo/tiger/GENZ2015/shp/cb_2015_us_state_20m.zip\", destfile = \"states.zip\")\nunzip(\"states.zip\")\nsf.us <- st_read(\"cb_2015_us_state_20m.shp\")## Reading layer `cb_2015_us_state_20m' from data source \n##   `/Users/bosche/Desktop/stat_764_portfolio/cb_2015_us_state_20m.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 52 features and 9 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -179.1743 ymin: 17.91377 xmax: 179.7739 ymax: 71.35256\n## Geodetic CRS:  NAD83\nsf.kansas <- sf.us[48,6]\nsf.kansas <- as(sf.kansas, 'Spatial')\n# Shapefile of study area at Agronomy North Farm - KSU (Manhattan KS)\nurl <- \"https://www.dropbox.com/scl/fi/umgo5u6ns6jkbbykqcq90/study_area.gpx?rlkey=63gjxiifvb4xq7ll25bfwyb8c&dl=1\"\npt.study.area <- st_read(dsn=url,layer=\"track_points\")## Reading layer `track_points' from data source \n##   `https://www.dropbox.com/scl/fi/umgo5u6ns6jkbbykqcq90/study_area.gpx?rlkey=63gjxiifvb4xq7ll25bfwyb8c&dl=1' \n##   using driver `GPX'\n## Simple feature collection with 122 features and 26 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -96.59424 ymin: 39.20581 xmax: -96.5938 ymax: 39.20647\n## Geodetic CRS:  WGS 84\nsf.study.area  <- st_polygon(list(rbind(st_coordinates(pt.study.area),st_coordinates(pt.study.area)[1,])))\nsf.study.area <- st_buffer(sf.study.area, .00006)\nsf.study.area <- st_sf(st_sfc(sf.study.area), crs = crs(sf.kansas))\n\nplot(sf.study.area)\nurl <- \"https://www.dropbox.com/scl/fi/1b5r111l0man0la5wr1vg/NF_mungbean_trial.gpx?rlkey=80dv7ghjwt57299bwip2t4iqz&dl=1\"\npt.elev <- st_read(dsn=url,layer=\"track_points\")## Reading layer `track_points' from data source \n##   `https://www.dropbox.com/scl/fi/1b5r111l0man0la5wr1vg/NF_mungbean_trial.gpx?rlkey=80dv7ghjwt57299bwip2t4iqz&dl=1' \n##   using driver `GPX'\n## Simple feature collection with 492 features and 26 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -96.59423 ymin: 39.20584 xmax: -96.59381 ymax: 39.20647\n## Geodetic CRS:  WGS 84\npt.elev <- pt.elev[,4] "},{"path":"activity-2.html","id":"plotmap-your-elevation-data.-i-would-recommend-using-r-andor-google-earth.","chapter":"5 Activity 2","heading":"5.4 Plot/map your elevation data. I would recommend using R and/or Google earth.","text":"","code":"\nggplot()+\n  geom_sf(data = pt.elev, aes(fill = ele ), shape = 21)+\n  scale_fill_viridis_c(option = \"plasma\", direction = -1)+\n  labs(x = 'Longitude', y = 'Latitude')+\n  theme_bw()+\n  theme(axis.text.x = element_text(angle = 20))+\n  labs(title = \"Elevation data:\",\n       subtitle = \"Field trial - Agronomy North Farm - KSU\",\n       fill = \"Elevation (m)\")"},{"path":"activity-2.html","id":"create-data-frame-elevation-longitude-latitude","chapter":"5 Activity 2","heading":"5.5 Create data frame: elevation, longitude, latitude","text":"data seems okay, major problems. ’d say wasn’t expect see 1 meter elevation change, however already knew field lower elevation towards east.","code":"\n# Transform data to a planar coordinate reference system\n\npt.elev.utm <- st_transform(pt.elev,CRS(\"+proj=utm +zone=14 +datum=WGS84  +units=m\"))\nsf.study.area.utm <- st_transform(sf.study.area,CRS(\"+proj=utm +zone=14 +datum=WGS84  +units=m\"))\n\n\n# Create data frame\ndf.elev <- data.frame (elev = pt.elev$ele,\n                          long = st_coordinates(pt.elev)[,1],\n                          lat = st_coordinates(pt.elev)[,2],\n                          s1 = st_coordinates(pt.elev.utm)[,1],\n                          s2 = st_coordinates(pt.elev.utm)[,2])\n# Summary statistics\n\nsummary(pt.elev$ele)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   333.2   333.5   333.8   333.8   334.2   334.7\n# Box Plot\n\nboxplot(pt.elev$ele, main = \"Elevation Boxplot\", ylab = \"Elevation\", col = \"navy\")\n# Histogram\nhist(pt.elev$ele,col=\"navy\", xlim = c(333, 335), breaks = 7, main=\"Histogram of elevation\",xlab=\"elevation (m)\")\n# Density Plot\nggplot() +\n  geom_density(aes(x = pt.elev$ele), fill = \"navy\", alpha = 0.8) +\n  xlim(333, 335)+\n  labs(title = \"Elevation Density Plot\", x = \"Elevation\", y = \"Density\")+\n  theme_bw()"},{"path":"activity-2.html","id":"write-out-the-goals-that-you-wish-to-accomplish-using-your-elevation-data.-for-example-my-goal-was-to-make-a-map-of-the-dickens-hall-parking-lot.-this-involves-using-the-elevation-data-i-collected-to-make-predictions-of-the-elevation-at-any-possible-spatial-locations-within-the-parking-lot.-i-would-also-like-to-make-inference-about-the-location-where-the-elevation-is-lowest-within-the-parking-lot.","chapter":"5 Activity 2","heading":"5.6 Write out the goals that you wish to accomplish using your elevation data. For example, my goal was to make a map of the Dicken’s Hall parking lot. This involves using the elevation data I collected to make predictions of the elevation at any possible spatial locations within the parking lot. I would also like to make inference about the location where the elevation is lowest within the parking lot.","text":"goal identify elevation change patterns specific field Agronomy North Farm, plan experiment year. First , wanna create elevation map field, map showing elevation variations across entire field. , want use elevation data pinpoint lowest elevation points within field. areas susceptible flooding may require special attention experiment planning (place experimental blocks within field). addition , make sure blocks situated areas minimal elevation variation minimize potential biases.","code":""},{"path":"activity-2.html","id":"write-out-several-statistical-or-machine-learning-models-that-you-think-you-can-use-to-answer-the-questionsgoals-you-wrote-in-prompt-5.-be-as-creative-and-inclusive-here.-for-each-statistical-or-machine-learning-model-make-sure-you-explain-each-component-piece-of-the-model.","chapter":"5 Activity 2","heading":"5.7 Write out several statistical or machine learning models that you think you can use to answer the questions/goals you wrote in prompt #5. Be as creative and inclusive here. For each statistical or machine learning model, make sure you explain each component (piece) of the model.","text":"Linear model:\\(y_i = \\beta_0 + \\beta_1\\cdot lon + \\beta_2\\cdot lon^2 + \\beta_3\\cdot lat + \\beta_4\\cdot lat^2 + \\epsilon_i\\)\\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\)GAM Model:\n\\(y_i= \\beta_0 + f(lon, lat) + \\epsilon_i\\)\\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\)","code":""},{"path":"activity-2.html","id":"of-the-models-you-developed-in-prompt-6-find-or-develop-software-to-fit-at-least-two-of-these-models-to-your-elevation-data.-note-that-in-a-perfect-world-you-would-be-able-to-either-find-existing-software-or-develop-new-programs-that-enable-you-to-fit-any-statistical-or-machine-learning-model-you-want.-in-reality-you-may-may-end-up-having-to-make-some-unwanted-changes-to-your-models-in-prompt-6-to-be-able-to-find-existing-software-to-fit-these-models-to-the-data.","chapter":"5 Activity 2","heading":"5.8 Of the models you developed in prompt #6, find (or develop) software to fit at least two of these models to your elevation data. Note that in a perfect world, you would be able to either find existing software or develop new programs that enable you to fit any statistical or machine learning model you want. In reality, you may may end up having to make some unwanted changes to your models in prompt #6 to be able to find existing software to fit these models to the data.","text":"","code":""},{"path":"activity-2.html","id":"model-1-non-hierarchical-linear-model-with-iid-errors","chapter":"5 Activity 2","heading":"5.8.1 Model 1: Non-hierarchical linear model with iid errors","text":"","code":"\n# Second order polynomial model\n\nm1 <- lm(elev~s1+I(s1^2)+s2+I(s2^2),data=df.elev)\n\n# Make raster of study area to be able to map predictions from m1\nrl.E.y <- raster(,nrow=100,ncols=100,ext=extent(sf.study.area.utm),crs=crs(sf.study.area.utm))\n\n# Make data.frame to be able to make predictions at each pixel (cell of raster)\ndf.pred <- data.frame(elev = NA,\n                      s1 = xyFromCell(rl.E.y,cell=1:length(rl.E.y[]))[,1],\n                      s2 = xyFromCell(rl.E.y,cell=1:length(rl.E.y[]))[,2])\n\n# Make spatial predictions at each pixel\ndf.pred$elev <- predict(m1,newdata=df.pred) \n\n\n# View first 6 rows of predictions\nhead(df.pred) ##       elev       s1      s2\n## 1 334.2992 707720.0 4342454\n## 2 334.2813 707720.5 4342454\n## 3 334.2633 707721.0 4342454\n## 4 334.2453 707721.5 4342454\n## 5 334.2274 707722.0 4342454\n## 6 334.2094 707722.5 4342454\n# Fill raster file with predictions \nrl.E.y[] <- c(df.pred$elev)\n\nrl.E.y <- mask(rl.E.y,sf.study.area.utm)\n\n# Plot map of predictions\nplot(rl.E.y, main = \"Second order polynomial regression\")\nplot(sf.study.area.utm,add=TRUE)"},{"path":"activity-2.html","id":"model-2-hierarchical-linear-model-with-two-error-terms-using-low-rank-approximation","chapter":"5 Activity 2","heading":"5.8.2 Model 2: Hierarchical linear model with two error terms using low-rank approximation","text":"","code":"\nm2 <- m2 <- gam(elev~s(s1,s2,bs=\"gp\",k=50,m=c(-2,10^4,2)),\n          family=gaussian(link = \"identity\"),\n          method = \"ML\",\n          data=df.elev)\n\nsummary(m2)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## elev ~ s(s1, s2, bs = \"gp\", k = 50, m = c(-2, 10^4, 2))\n## \n## Parametric coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 3.338e+02  3.098e-03  107775   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##            edf Ref.df    F p-value    \n## s(s1,s2) 4.956      6 2851  <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Rank: 7/50\n## R-sq.(adj) =  0.972   Deviance explained = 97.2%\n## -ML = -594.17  Scale est. = 0.0047207  n = 492\n# Make raster of study area to be able to map predictions from m3\nrl.E.y <- raster(,nrow=100,ncols=100,ext=extent(sf.study.area.utm),crs=crs(sf.study.area.utm))\n\n\n# Make data.frame to be able to make predictions at each pixel (cell of raster)\ndf.pred <- data.frame(elev = NA,\n                      s1 = xyFromCell(rl.E.y,cell=1:length(rl.E.y[]))[,1],\n                      s2 = xyFromCell(rl.E.y,cell=1:length(rl.E.y[]))[,2])\n\n# Make spatial predictions at each pixel\ndf.pred$elev <- c(predict(m2,newdata=df.pred,type=\"response\"))\n\n# View first 6 rows of predictions\nhead(df.pred) ##       elev       s1      s2\n## 1 334.2379 707720.0 4342454\n## 2 334.2175 707720.5 4342454\n## 3 334.1974 707721.0 4342454\n## 4 334.1774 707721.5 4342454\n## 5 334.1576 707722.0 4342454\n## 6 334.1380 707722.5 4342454\n# Fill raster file with predictions \nrl.E.y[] <- df.pred$elev\n\nrl.E.y <- mask(rl.E.y,sf.study.area.utm)\n\n# Plot map of predictions\nplot(rl.E.y, main = \"Kriging\")\nplot(sf.study.area.utm,add=TRUE)"},{"path":"activity-2.html","id":"related-to-prompt-5-use-both-models-you-fit-to-your-elevation-data-in-prompt-7-to-answer-the-questionsgoals.","chapter":"5 Activity 2","heading":"5.9 Related to prompt #5, use both models you fit to your elevation data in prompt #7 to answer the questions/goals.","text":"Based results, since field shows clear lower elevation trend towards east, can assign blocks west east, way minimize bias due elevation changes.Using second model, seems predict better elevation pattern:","code":"\n# Make spatial predictions at each pixel\ndf.pred$elev <- c(predict(m1,newdata=df.pred,type=\"response\"))\n\n# Fill raster file with predictions \nrl.E.y[] <- df.pred$elev\n\n# Estimate coordinates and point of minimum elevation\nxyFromCell(rl.E.y,cell=which.min(rl.E.y[]))##             x       y\n## [1,] 707769.2 4342454\nrl.E.y[which.min(rl.E.y[])]## [1] 332.5205\n# Plot estimate coordinates of maximum elevation\n\nrl.E.y <- mask(rl.E.y,sf.study.area.utm)\nplot(rl.E.y)\nplot(sf.study.area.utm,add=TRUE)\npoints(xyFromCell(rl.E.y,cell=which.min(rl.E.y[])),col=\"tomato\",pch=\"x\",cex=2)"},{"path":"activity-2.html","id":"based-on-the-material-in-chapter-6-of-spatio-temporal-statistics-with-r-and-our-discussion-in-class-on-march-26-compare-check-and-evaluate-the-two-models-from-8","chapter":"5 Activity 2","heading":"5.10 Based on the material in Chapter 6 of Spatio-Temporal Statistics with R and our discussion in class on March 26, compare, check and evaluate the two models from #8","text":"","code":"\n# Create train and test data\nset.seed(123)\ntrainIndices <- sample(1:nrow(df.elev), size = floor(0.7 * nrow(df.elev)))\n\n# Subset the data into train and test sets\ntrain_df <- df.elev[trainIndices, ]\ntest_df <- df.elev[-trainIndices, ]"},{"path":"activity-2.html","id":"fit-m1-using-train-df","chapter":"5 Activity 2","heading":"5.11 Fit m1 using train df","text":"","code":"\n# Second order polynomial model\n\nm1 <- lm(elev~s1+I(s1^2)+s2+I(s2^2),data=train_df)\n\n# Make raster of study area to be able to map predictions from m1\nrl.E.y <- raster(,nrow=100,ncols=100,ext=extent(sf.study.area.utm),crs=crs(sf.study.area.utm))\n\n# Make data.frame to be able to make predictions at each pixel (cell of raster)\ndf.pred <- data.frame(elev = NA,\n                      s1 = xyFromCell(rl.E.y,cell=1:length(rl.E.y[]))[,1],\n                      s2 = xyFromCell(rl.E.y,cell=1:length(rl.E.y[]))[,2])\n\n# Make spatial predictions at each pixel\ndf.pred$elev <- predict(m1,newdata=df.pred) \n\n\n# View first 6 rows of predictions\nhead(df.pred) ##       elev       s1      s2\n## 1 334.3000 707720.0 4342454\n## 2 334.2822 707720.5 4342454\n## 3 334.2645 707721.0 4342454\n## 4 334.2467 707721.5 4342454\n## 5 334.2289 707722.0 4342454\n## 6 334.2112 707722.5 4342454\n# Fill raster file with predictions \nrl.E.y[] <- c(df.pred$elev)\n\nrl.E.y <- mask(rl.E.y,sf.study.area.utm)\n\n# Plot map of predictions\nplot(rl.E.y, main = \"Second order polynomial regression\")\nplot(sf.study.area.utm,add=TRUE)"},{"path":"activity-2.html","id":"fit-m2-using-train-df","chapter":"5 Activity 2","heading":"5.12 Fit m2 using train df","text":"","code":"\nm2 <- m2 <- gam(elev~s(s1,s2,bs=\"gp\",k=50,m=c(-2,10^4,2)),\n          family=gaussian(link = \"identity\"),\n          method = \"ML\",\n          data=train_df)\n\nsummary(m2)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## elev ~ s(s1, s2, bs = \"gp\", k = 50, m = c(-2, 10^4, 2))\n## \n## Parametric coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 3.338e+02  3.776e-03   88396   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##           edf Ref.df    F p-value    \n## s(s1,s2) 4.94      6 1903  <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Rank: 7/50\n## R-sq.(adj) =  0.971   Deviance explained = 97.1%\n## -ML = -402.09  Scale est. = 0.0049059  n = 344\n# Make raster of study area to be able to map predictions from m3\nrl.E.y <- raster(,nrow=100,ncols=100,ext=extent(sf.study.area.utm),crs=crs(sf.study.area.utm))\n\n\n# Make data.frame to be able to make predictions at each pixel (cell of raster)\ndf.pred <- data.frame(elev = NA,\n                      s1 = xyFromCell(rl.E.y,cell=1:length(rl.E.y[]))[,1],\n                      s2 = xyFromCell(rl.E.y,cell=1:length(rl.E.y[]))[,2])\n\n# Make spatial predictions at each pixel\ndf.pred$elev <- c(predict(m2,newdata=df.pred,type=\"response\"))\n\n# View first 6 rows of predictions\nhead(df.pred) ##       elev       s1      s2\n## 1 334.2387 707720.0 4342454\n## 2 334.2183 707720.5 4342454\n## 3 334.1981 707721.0 4342454\n## 4 334.1781 707721.5 4342454\n## 5 334.1582 707722.0 4342454\n## 6 334.1386 707722.5 4342454\n# Fill raster file with predictions \nrl.E.y[] <- df.pred$elev\n\nrl.E.y <- mask(rl.E.y,sf.study.area.utm)\n\n# Plot map of predictions\nplot(rl.E.y, main = \"Kriging\")\nplot(sf.study.area.utm,add=TRUE)"},{"path":"activity-2.html","id":"compare-point-prediction-of-the-the-expected-value-of-elevation-ey-to-observed-records-from-test-data-set.","chapter":"5 Activity 2","heading":"5.13 Compare point prediction of the the expected value of elevation (E(y)) to observed records from test data set.","text":"","code":""},{"path":"activity-2.html","id":"m1","chapter":"5 Activity 2","heading":"5.13.1 m1","text":"","code":"\nE.y.m1 <- predict(m1,newdata=test_df)\nplot(E.y.m1,test_df$elev,xlab=\"Predicted expected value\",ylab=\"New observed elevation\")\n# Quantify predictive accuracy using scoring rule \n\nsum(dnorm(test_df$elev,E.y.m1,log=TRUE)) # logarithmic scoring rule (proper scoring rule for linear model)## [1] -136.5505\nmean((test_df$elev - E.y.m1)^2) # Mean square error. Commonly used scoring rule, but not proper scoring rule for ALL situations## [1] 0.00739952\nmean(abs(test_df$elev - E.y.m1)) # Mean absolute error. Commonly used scoring rule, but not proper scoring rule for ALL situations## [1] 0.06668116\n# Quantify calibration of predictive intervals. \n\nPI.m1 <- predict(m1,newdata=test_df,\n               interval = c(\"prediction\"),\n               level = 0.95)\n\n# Determine what % of the new observations fall within the prediction intervals.\n\nmean(ifelse(test_df$elev>PI.m1[,2],1,0)*ifelse(test_df$elev<PI.m1[,3],1,0))## [1] 0.9662162"},{"path":"activity-2.html","id":"m2","chapter":"5 Activity 2","heading":"5.13.2 m2","text":"summary, comparing models, m2 (GAM) best model, presenting lower logarithmic scoring rule, MSE MAE.","code":"\nE.y.m2 <- predict(m2,newdata=test_df)\nplot(E.y.m2,test_df$elev,xlab=\"Predicted expected value\",ylab=\"New observed elevation\")\n# Quantify predictive accuracy using scoring rule \n\nsum(dnorm(test_df$elev,E.y.m2,log=TRUE)) # logarithmic scoring rule (proper scoring rule for linear model)## [1] -136.3232\nmean((test_df$elev - E.y.m2)^2) # Mean square error. Commonly used scoring rule, but not proper scoring rule for ALL situations## [1] 0.004328524\nmean(abs(test_df$elev - E.y.m2)) # Mean absolute error. Commonly used scoring rule, but not proper scoring rule for ALL situations## [1] 0.05134698\n# Quantify calibration of predictive intervals. \n\nPI.m2 <- predict(m2,newdata=test_df,\n               interval = c(\"prediction\"),\n               level = 0.95)"},{"path":"activity-3.html","id":"activity-3","chapter":"6 Activity 3","heading":"6 Activity 3","text":"","code":""},{"path":"activity-3.html","id":"library-2","chapter":"6 Activity 3","heading":"6.1 Library","text":"","code":"\nlibrary(sf)\nlibrary(sp)\nlibrary(raster)\nlibrary(mgcv)\nlibrary(ggplot2)\nlibrary(dplyr)"},{"path":"activity-3.html","id":"import-ks-map-and-aphid-data","chapter":"6 Activity 3","heading":"6.2 Import KS map and aphid data","text":"","code":"\n# Download data on English grain aphid\n\nurl <- \"https://www.dropbox.com/scl/fi/9ymxt900s77uq50ca6dgc/Enders-et-al.-2018-data.csv?rlkey=0rxjwleenhgu0gvzow5p0x9xf&dl=1\"\ndf1 <- read.csv(url)\ndf1 <- df1[,c(2,8:10)] # Keep only the data on English grain aphid\n\npts.sample <- data.frame(long = df1$long,lat = df1$lat, \n                         count = df1$EGA)\ncoordinates(pts.sample) =~ long + lat\nproj4string(pts.sample) <- CRS(\"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\")"},{"path":"activity-3.html","id":"data-visualization","chapter":"6 Activity 3","heading":"6.3 Data visualization","text":"##1. data abundance English grain aphids, propose three different statistical models (machine learning approach) capable predicting number English grain aphids location within state Kansas time years 2014 2015. Make sure write three statistical models using formal notation fully describe component using words.","code":"\n# Download KS shapefile\nks <- raster::getData(name=\"GADM\", country=\"USA\", level=1) %>% \n  st_as_sf() %>% \n  dplyr::filter(NAME_1 == \"Kansas\")\n\ncoordinates <- st_as_sf(df1, coords = c(\"long\", \"lat\"), crs = st_crs(ks))\n\n# Plot\nggplot() +\n  geom_sf(data = ks, fill = \"grey90\", color = \"black\") +\n  geom_point(data = df1, aes(x = long, y = lat, size = EGA), shape = 21) +\n  #scale_fill_viridis_c (option = \"viridis\", direction = -1) +\n  labs(title = \"Abundance of English Grain Aphids\", x = \"Longitude\", y = \"Latitude\")+\n  facet_wrap(~year)\nhist(df1$EGA)\n# Download National Land Cover Database\nurl.nlcd <- \"https://www.dropbox.com/scl/fi/ew7yzm93aes7l8l37cn65/KS_2011_NLCD.img?rlkey=60ahyvxhq18gt0yr47tuq5fig&dl=1\"\nrl.nlcd2011 <- raster(url.nlcd)\n\n# Make raster file that contains pixels with value of 1 if grassland and \n# zero if other type of land cover.\nplot(rl.nlcd2011)\nrl.nlcd.grass <- rl.nlcd2011\nrl.nlcd.grass[] <- ifelse(rl.nlcd.grass[]==71,1,0)\n\nplot(rl.nlcd.grass)\n# Calculate percentage of land area that is grassland withing 5 km of sampled location\ndf1$grass.perc <- unlist(lapply(extract(rl.nlcd.grass,pts.sample,buffer=5000),mean))*100\n\nhist(df1$grass.perc,col=\"grey\",main=\"\",xlab=\"% grassland within \\n5 km at sample location\")"},{"path":"activity-3.html","id":"model-1","chapter":"6 Activity 3","heading":"6.3.1 Model 1","text":"Data model\n\\[Z = y\\]\nProcess model\nUsing Poisson distribution\\[[y|\\lambda] = Poisson(\\lambda) \\]\\[\\eta_s\\sim MVN(0, \\Sigma)\\]\n\\[E(y)=e^{\\beta_0+\\beta_1\\cdot X+\\eta_s+\\eta_t}\\]","code":""},{"path":"activity-3.html","id":"model-2","chapter":"6 Activity 3","heading":"6.3.2 Model 2","text":"Data model\\[Z = y\\]\nProcess model\nUsing Negative binomial distribution\\[[y|r,p] = NB(r, p)\\]\n\\[\\eta_s\\sim MVN(0, \\Sigma)\\]\n\\[E(y)=e^{\\beta_0+\\beta_1\\cdot X+\\eta_s+\\eta_t}\\]\n### Model 3Data model\\[Z = y\\]\nProcess model\nUsing zero inflated poisson distribution\\[[y|p, \\lambda]=ZIP(p,\\lambda)\\]\n\\[\\eta_s\\sim MVN(0, \\Sigma)\\]\n\\[E(y)=e^{\\beta_0+\\beta_1\\cdot X+\\eta_s+\\eta_t}\\]##2. three statistical models proposed question #1, propose way measure accuracy (perhaps calibration) predictions.Mean square error Mean absolute error\\[MSE = \\frac{1}{n} \\sum^n_{=1}{(Y_i - \\hat{Y}_i)^2}\\]\\[MAE = \\frac{\\sum^n_{=1} |y_i - x_i|}{n}\\]\n##3. Fit three statistical models proposed question #1 English grain aphid abundance data.##4. three models fit question #3, model makes accurate predictions? good best model real world terms? Remember trying predict number English grain aphids, count!Model 2 makes accurate predictions.##5. Summarize results using words, numerical values figures/maps.","code":"\nm1 <- gam(EGA ~ grass.perc + as.factor(year) + s(long,lat, bs = \"gp\"), \n          family = poisson(link = \"log\"), data = df1)\n\nsummary(m1)## \n## Family: poisson \n## Link function: log \n## \n## Formula:\n## EGA ~ grass.perc + as.factor(year) + s(long, lat, bs = \"gp\")\n## \n## Parametric coefficients:\n##                       Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)         -3.1624342  0.2629681 -12.026   <2e-16 ***\n## grass.perc          -0.0084544  0.0009811  -8.617   <2e-16 ***\n## as.factor(year)2015  5.5241770  0.2584600  21.373   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##               edf Ref.df Chi.sq p-value    \n## s(long,lat) 31.97     32   8009  <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.395   Deviance explained = 69.9%\n## UBRE = 28.333  Scale est. = 1         n = 341\nm2 <- gam(EGA ~ grass.perc + as.factor(year) + s(long,lat, bs = \"gp\"), \n          family = nb(theta = NULL,link = \"log\"), data = df1)\n\nsummary(m2)## \n## Family: Negative Binomial(0.623) \n## Link function: log \n## \n## Formula:\n## EGA ~ grass.perc + as.factor(year) + s(long, lat, bs = \"gp\")\n## \n## Parametric coefficients:\n##                      Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)         -2.512753   0.343910  -7.306 2.74e-13 ***\n## grass.perc          -0.005170   0.004665  -1.108    0.268    \n## as.factor(year)2015  5.164253   0.325909  15.846  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##               edf Ref.df Chi.sq p-value    \n## s(long,lat) 8.884  11.75  372.1  <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.247   Deviance explained = 68.6%\n## -REML = 962.32  Scale est. = 1         n = 341\nm3 <- gam(list(EGA ~ grass.perc + as.factor(year) + s(long,lat, bs = \"gp\"), ~ s(long,lat, bs = \"gp\")), \n          family = ziplss(), data = df1)\n\nsummary(m3)## \n## Family: ziplss \n## Link function: identity identity \n## \n## Formula:\n## EGA ~ grass.perc + as.factor(year) + s(long, lat, bs = \"gp\")\n## ~s(long, lat, bs = \"gp\")\n## \n## Parametric coefficients:\n##                       Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)         -1.5217298  0.3565564  -4.268 1.97e-05 ***\n## grass.perc          -0.0098653  0.0009923  -9.942  < 2e-16 ***\n## as.factor(year)2015  4.0446085  0.3513845  11.510  < 2e-16 ***\n## (Intercept).1        0.0363224  0.0745315   0.487    0.626    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                 edf Ref.df  Chi.sq  p-value    \n## s(long,lat)   31.55  31.84 6583.32  < 2e-16 ***\n## s.1(long,lat) 12.04  15.69   43.33 0.000236 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Deviance explained = 61.3%\n## -REML = 5365.4  Scale est. = 1         n = 341\nAIC(m1, m2, m3)##          df       AIC\n## m1 34.97341 10896.425\n## m2 13.90150  1913.024\n## m3 49.13742 10462.936\n# Predict\n\nnewPoints <- st_sample(ks, size = 1000, type = \"regular\") %>% \n  as(., 'Spatial') %>% as.data.frame() %>% \n    rename(\"long\" = \"coords.x1\", \n          \"lat\" = \"coords.x2\") %>% \n  cross_join(data.frame(year = as.factor(c('2014', '2015'))))\n\npts.sample<- newPoints\n\ncoordinates(pts.sample) =~ long + lat\nproj4string(pts.sample) <- CRS(\"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\")\n\n# Calculate percentage of land area that is grassland withing 5 km of new points location\n\nnewPoints$grass.perc <- unlist(lapply(extract(rl.nlcd.grass,pts.sample,buffer=5000),mean))*100\n\n# Step 2: Use predict() to obtain predictions\nnewPoints$predictions.m1 <- predict(m1, newdata = newPoints, type = \"response\")\nnewPoints$predictions.m2 <- predict(m2, newdata = newPoints, type = \"response\")\nnewPoints$predictions.m3 <- predict(m3, newdata = newPoints, type = \"response\")\n\n\n# View the predictions\npred <- st_as_sf(newPoints, coords = c(\"long\", \"lat\"), crs = st_crs(ks), agr = 'constant')\nAIC(m1, m2, m3)##          df       AIC\n## m1 34.97341 10896.425\n## m2 13.90150  1913.024\n## m3 49.13742 10462.936\n# m1\nggplot() +\n  geom_tile(data = newPoints, aes(x = long, y = lat, fill = predictions.m1))+\n  scale_fill_viridis_c(values = c(0, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1))+ \n  labs(title = \"Model 1 - Abundance of English grain aphids\", x = \"Longitude\", y = \"Latitude\")+\n  theme_bw()\n# m2\nggplot() +\n  geom_tile(data = newPoints, aes(x = long, y = lat, fill = predictions.m2))+\n  scale_fill_viridis_c()+ \n  labs(title = \"Model 2 - Abundance of English grain aphids\", x = \"Longitude\", y = \"Latitude\")+\n  theme_bw()\n#m3 \nggplot() +\n  geom_tile(data = newPoints, aes(x = long, y = lat, fill = predictions.m3))+\n  scale_fill_viridis_c(values = c(0, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1))+ \n  labs(title = \"Model 3 - Abundance of English grain aphids\", x = \"Longitude\", y = \"Latitude\")+\n  theme_bw()"}]
